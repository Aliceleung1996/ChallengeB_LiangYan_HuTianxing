---
title: "Challenge B - Liang Yan & Hu Tianxing"
author: "Liang Yan & Hu Tianxing"
date: "2017/12/7"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# load all the packages needed in the the beginning

```{r packages}
library(np)
library(tidyverse)
library(MASS)
library(tidyr)
library(dplyr)
library(caret)
library(readxl)
```

# input the training data and the test data
```{r input data}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

# From Challenge A, we found that while NA in some variables indeed means "missing value".
# While some NA in other variables has practical meanings. 
# We removed all the NAs from numeric variables and two character varialbes “Electrical” and “MasVnrType”, where NA has no practical meanings.

```{r removes NA}
train <- train[-c(1,7,31,32,33,34,36,58,59,61,64,65,73,74,75)]
train
test <- tbl_df(test[-c(1,7,31,32,33,34,36,58,59,61,64,65,73,74,75)])
test
train <- train[complete.cases(train),]
train
sum(is.na(train))
train
test <- test[complete.cases(test),]
test
```

# train1 is not the training data. It is just used to create the formula in a convenient way by removing the dependent variable "SalePrice".
```{r create formula}
train1 <- tbl_df(train[-c(66)])
names(train1)
fmla <- as.formula(paste("SalePrice ~", paste(names(train1), collapse = "+")))
fmla
```

# A  non-parametric statistical method makes no assumption on the population distribution or sample size. 
# Generally people make as few assumptions about the regression function f as possible and as much data as possible to learn about the potential shape of f to make f very flexible yet smooth. 
# We choose Local Linear Method in the np package because it achieves such flexibility by fitting a different, simple model separately at every point. 
# Unlike kernel regression, locally linear estimation would have no bias if the true model were linear. In general, locally linear estimation removes a bias term from the kernel estimator, that makes it have better behavior near the boundary of the x’s and smaller MSE everywhere.

```{r}
model <- npreg(fmla, data = train, regtype = "ll", bwmethod = "cv.aic")
summary(model)
pred <- predict(model, test)
```

# This is the linear model we estimated in challenge A after comparing the exclusion of missing values and the comparison of the importance of variables.
# It has a R-squared of 0.8347. So we conclude that it is not as good as the predictions using the nonparametric local linear model.
```{r}
lm_tra <- lm(SalePrice ~ LotFrontage +OverallQual 
             +RoofMatl+ MasVnrArea+   BsmtFinSF1   
             + BsmtFinSF2 + BsmtUnfSF+ X1stFlrSF +X2ndFlrSF +KitchenQual,data=train)
summary(lm_tra)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
